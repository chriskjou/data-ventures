{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview for Week 5\n",
    "We're going to be dealing with a lot of probability and some math this week, so it would be helpful to go over some basic facts before starting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Random Variables ###\n",
    "\n",
    "A random variable is simply a variable whose value depends on a random event. For example, let $X$ be the number from a roll of a die. Then $X$ can be thought of as a random variable that takes on values in $\\{1,2,3,4,5,6\\}$. We can come up with much more complicated random variables, though. In general, random variables can be either discrete or continous, or a mixture of both. \n",
    "\n",
    "Each random variable is associated with a probability **distribution**. For discrete random variables, we define the distribution as the probability $P(X = z)$ for every possible $z$. This is known as the Probability Mass Function (PMF). The set of possible values for $z$ is known as the **support** of the distribution. For continous distributions, we talk about a distribution function $f(z)$ where $f(z)dz$ is the probability of a value in a small interval around z. \n",
    "\n",
    "If we want to say that $X$ has a particular distribution $D$, we denote this $X \\sim D$.  Of particular importance is the Normal (or Gaussian) distribution. This is the most widely studied distribution in probability and statistics (notably, under some moderate restrictions, large sums of random variables will always converge to be distributed as a Normal). We usually denote a normal random variable as $Z$ and write $Z \\sim N(\\mu, \\sigma^2)$. More on this later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Conditional Probability and Bayes Rule \n",
    "\n",
    "Now that we know what random variables are, we can ask questions about probability such as: what is the probability $X = z$, or $P(X=z)$. What if two random variables are related in such a way that the probability distribution of one depends on the outcome of another? To make this concrete, consider the probability that classes are cancelled given the weather. Intuitively, the distribution of classes being cancelled depends on the weather. We denote the probability of some event $A$, given another event $B$ has occurred, as $P(A|B)$. \n",
    "\n",
    "Definition of Conditional Probability:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(A\\cap B)}{P(B)}$$\n",
    "\n",
    "Where $A\\cap B$ is the event that both $A$ and $B$ occur. Think about why this makes sense as a definition. It is the probability that both $A$ and $B$ occur, accounting for the fact that we know $B$ has occured by dividing by the probability $B$ occurs.\n",
    "\n",
    "Bayes Rule:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "Bayes Rule is a way of relating $P(A|B)$ to $P(B|A)$ and comes directly from this definition.\n",
    "\n",
    "**proof)**\n",
    "\n",
    "Writing the definition of conditional probability in two ways: \n",
    "$$ P(A|B) = \\frac{P(A\\cap B)}{P(B)}$$\n",
    "$$P(B|A) = \\frac{P(A\\cap B)}{P(A)}$$\n",
    "\n",
    "So \n",
    "$$P(B|A)P(A) = P(A|B)P(B) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Expected Value ###\n",
    "\n",
    "The expected value of a random variable, $X$, is the weighted average of all possible values $X$ could take on, weighted by the probability of that value. For a discrete random variable this is:\n",
    "$$E(X) = \\sum_z z * P(X=z) $$\n",
    "\n",
    "For a continous random variable, analagously this is:\n",
    "$$E(X) = \\int_{-\\infty}^\\infty z * f(z) dz $$\n",
    "\n",
    "Example (One way of thinking about expected value): Suppose you play game where you pay \\$ $D$ dollars, roll a die, and the amount that you get back in dollars is the number on the die. What value should $D$ be so that if you played this game infinitely many times, you would not lose or win any money?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Joint Distributions\n",
    "For a discrete random variable, the joint distribution of $X$ and $Y$ is:\n",
    "\n",
    "$$P(X=x \\cap Y=y) $$\n",
    "\n",
    "For a continous random variable, the joint distribution is analagously:\n",
    "\n",
    "$$f_{X,Y}(x,y)$$\n",
    "\n",
    "where $f_{X,Y}(x,y) dx dy$ is the probability of $(X,Y)$ in some small interval around $(x,y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Maximum Likelihood Estimation\n",
    "\n",
    "Maximum Likelihood Estimation is an approach to questions like this: suppose we have some random variables $X_1,X_2, \\dots X_n$ but we don't know their distribution. For now assume they all come from the same distribution. We observe $n$ outcomes: $x_1, x_2, \\dots, x_n$. Now we want to guess what distribution the $x_i$s came from, the distribution of each of the $X_i$s.\n",
    "\n",
    "Usually the way we do this is making assumptions about the distribution of $X$, and then choosing the parameters that maximize the likelihood of seeing that data. \n",
    "\n",
    "As an example, suppose we see the 10 outcomes of a coin flip. $X$ takes on the value $1$ in the case of heads, and $0$ in the case of tails so $X$ is a random variable with support $\\{0,1\\}$. Suppose we observe: $0,0,0,0,1,1, 0, 1, 1,1$. \n",
    "\n",
    "Well, a reasonable assumption is that the distribution of $X$ is as follows: there is a $p$ probability of heads, and a $1-p$ probability of tails. Our goal in doing maximum likelihood estimation then is to learn the value for $p$ that maximizes the likelihood of seeing $0,0,0,0,1,1, 0, 1, 1,1$. \n",
    "\n",
    "What is that likelihood? Note: in general, likelihood is the term for the probability when we are dealing with a realized random variable and then treating it as a function of the parameters of the distribution (in this case, $p$). \n",
    "\n",
    "We have $$L(p) = (1-p)^a (p)^b $$\n",
    "\n",
    "Where $a$ is the number of heads, and $b$ is the number of tails. Let's maximize this likelihood (using AP Calc material). First we take the log,which makes the calculation much nicer(note that log(x) is a increasing function so it preserves the argument that maximizes the function). \n",
    "\n",
    "$$\\log L(p) = a \\log(1-p) + b\\log(p) $$\n",
    "Taking the derivative:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial p} \\log L(p) =  - a \\frac{1}{1-p} + b \\frac{1}{p}$$\n",
    "\n",
    "Setting the derivative to 0 and simplifying a little, we have: \n",
    "$$\\frac{1-p}{p} = \\frac{a}{b} $$\n",
    "\n",
    "$$\\frac{1}{p} = \\frac{a}{b} + 1 $$\n",
    "\n",
    "$$p = \\frac{a}{a+b} $$\n",
    "\n",
    "Think about whether this final answer makes sense: the value for $p$ that maximizes the likelihood of the data is the number of heads over the total number of flips! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
